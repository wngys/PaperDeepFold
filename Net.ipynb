{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# kernel_List = [12, 4, 4, 4, 4, 4]\n",
    "# channel_List = [128, 256, 512, 512, 512, 400]\n",
    "\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channel, out_channel, kernel_sz, padding, stride = 2) -> None:\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Conv2d(in_channel, out_channel, kernel_sz, stride, padding)\n",
    "#         self.bn = nn.BatchNorm2d(out_channel)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout()\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.bn(self.conv(x)))\n",
    "#         x = self.dropout(x)\n",
    "#         return x\n",
    "\n",
    "# def get_convBlocks(in_channel):\n",
    "#     layerNum = len(kernel_List)\n",
    "#     blocks = []\n",
    "#     blocks.append(ConvBlock(in_channel, channel_List[0], kernel_List[0], int(kernel_List[0] / 2 - 1)))\n",
    "#     for i in range(1, layerNum):\n",
    "#         blocks.append(ConvBlock(channel_List[i-1], channel_List[i], kernel_List[i], int(kernel_List[i] / 2 - 1)))\n",
    "#     return blocks\n",
    "\n",
    "# class DeepFold(nn.Module):\n",
    "#     def __init__(self, in_channel) -> None:\n",
    "#         super().__init__()\n",
    "#         self.convLayer = nn.Sequential(*get_convBlocks(in_channel))\n",
    "    \n",
    "#     # [batch_size, 3, 256, 256]\n",
    "#     def forward(self, x):\n",
    "#         x = self.convLayer(x) # [batch_size, 400, 4, 4]\n",
    "#         x = torch.diagonal(x, dim1=2, dim2=3) # [batch_size, 400, 4]\n",
    "#         x = torch.mean(x, dim= 2) # [batch_size, 400]\n",
    "\n",
    "#         normValue = torch.norm(x, dim = 1) # norm_value [batch_size]\n",
    "#         # print(normValue.shape)\n",
    "#         # [400, batch_size]  除法要求最后一维要和norm_value维度匹配\n",
    "#         x = x.permute(1, 0)\n",
    "#         # [400, batch_size] 已经正则化\n",
    "#         x = torch.div(x, normValue)\n",
    "\n",
    "#         # [batch_size, 400]\n",
    "#         x = x.permute(1, 0)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_List = [12, 4, 4, 4, 4, 4]\n",
    "channel_List = [128, 256, 512, 512, 512, 400]\n",
    "\n",
    "class ConvBlock(nn.Module):   \n",
    "    def __init__(self, in_channel, out_channel, kernel_sz, padding, stride = 2) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_sz, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "def get_convBlocks(in_channel):\n",
    "    layerNum = len(kernel_List)\n",
    "    blocks = []\n",
    "    blocks.append(ConvBlock(in_channel, channel_List[0], kernel_List[0], int(kernel_List[0] / 2 - 1)))\n",
    "    for i in range(1, layerNum):\n",
    "        blocks.append(ConvBlock(channel_List[i-1], channel_List[i], kernel_List[i], int(kernel_List[i] / 2 - 1)))\n",
    "    return blocks\n",
    "\n",
    "class DeepFold(nn.Module):\n",
    "    def __init__(self, in_channel) -> None:\n",
    "        super().__init__()\n",
    "        self.convLayer = nn.Sequential(*get_convBlocks(in_channel))\n",
    "    \n",
    "    # [batch_size, 3, 256, 256] -> [batch_size, 400]\n",
    "    def forward(self, x):\n",
    "        x = self.convLayer(x) # [batch_size, 400, 4, 4]\n",
    "        x = torch.diagonal(x, dim1=2, dim2=3) # [batch_size, 400, 4]\n",
    "        x = torch.mean(x, dim= 2) # [batch_size, 400]\n",
    "        x = F.normalize(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原数据应是取逆扩充channel后的距离矩阵，而且inf项被替换\n",
    "def build_transform(in_channel):\n",
    "    train_tfm = T.Compose(\n",
    "        [\n",
    "            T.Resize((256, 256)),\n",
    "            # 取逆矩阵 扩充channel\n",
    "            # Pretfm(in_channel),\n",
    "            #是否需要数据增强 保留一个问号\n",
    "            # 层归一化\n",
    "            # nn.LayerNorm((in_channel, 256, 256)) # 不能经过LayNorm等网络层，不然输出数据 requires_grad = True,从而报错，原始数据应该为False\n",
    "            T.Normalize(mean=[0.0068, 0.0003, 2.3069e-05], std=[0.0140, 0.0015, 0.0002])\n",
    "        ]\n",
    "    )\n",
    "    return train_tfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_set(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dict_data, id_list, tfm) -> None:\n",
    "        super().__init__()\n",
    "        self.tensor_list = []\n",
    "        for id, label in id_list:\n",
    "            # 在蛋白质数据库文件查找 id.npy\n",
    "            # feature = torch.from_numpy(np.load(dir+id+\".npy\", allow_pickle=True))\n",
    "            feature = torch.from_numpy(dict_data[id])\n",
    "            feature = feature.to(torch.float) # torch.float = torch.float32\n",
    "            label = float(label)\n",
    "            self.tensor_list.append((feature,\n",
    "                                        label)\n",
    "                                        )\n",
    "        self.tfm = tfm\n",
    "\n",
    "    def __getitem__(self, idx :int):\n",
    "        y = self.tensor_list[idx][0]\n",
    "        y = self.tfm(y)\n",
    "        label = torch.tensor(self.tensor_list[idx][1], dtype=torch.float32)\n",
    "        return y, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取左侧一列id 对应的右侧id_list\n",
    "def get_id_list(pair_path):\n",
    "    id_list = []\n",
    "    with open(pair_path, \"r\") as f_r:\n",
    "        while True:\n",
    "            lines = f_r.readline()\n",
    "            if not lines:\n",
    "                break\n",
    "            line1= lines.split('\\t')[0]\n",
    "            line2 = lines.split('\\t')[1].split(\"\\n\")[0]\n",
    "            id_list.append((line1, line2))\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 获取id对应的distance_matrix\n",
    "def get_feature(dict_data, id, tfm):\n",
    "    feature = torch.from_numpy(dict_data[id])\n",
    "    feature = feature.to(torch.float)\n",
    "    feature = tfm(feature)\n",
    "    feature = feature.unsqueeze(0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max_margin_loss(nn.Module):\n",
    "    def __init__(self, K, m) -> None:\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.m = m\n",
    "   \n",
    "    def forward(self, fingerpvec1, fingerpvec2, label):\n",
    "        posi_vec_list = []\n",
    "        nega_vec_list = []\n",
    "        for number_inbatch in range(fingerpvec2.shape[0]):\n",
    "            if label[number_inbatch] == 0:\n",
    "                nega_vec_list.append(fingerpvec2[number_inbatch])\n",
    "            elif label[number_inbatch] == 1:\n",
    "                posi_vec_list.append(fingerpvec2[number_inbatch])\n",
    "            else:\n",
    "                print(\"ERROR\")\n",
    "\n",
    "        posi_cos_smi_list = []\n",
    "        nega_cos_smi_list = []\n",
    "        for posi_vec in posi_vec_list:\n",
    "            # print(\"posi_vec: \", posi_vec.shape)\n",
    "            posi_cos_smi_list.append(F.cosine_similarity(fingerpvec1, posi_vec, dim = 0))\n",
    "        for nega_vec in nega_vec_list:\n",
    "            nega_cos_smi_list.append(F.cosine_similarity(fingerpvec1, nega_vec, dim = 0))\n",
    "\n",
    "        posi_cos_smi_list.sort() # 升序排序 选最小\n",
    "        nega_cos_smi_list.sort(reverse=True) # 降序排序 选最大\n",
    "        posi_cos = posi_cos_smi_list[0] # 只选取一个正例\n",
    "        loss = 0\n",
    "        for i in range(self.K):\n",
    "            nega_cos = nega_cos_smi_list[i]\n",
    "            loss += max(0, nega_cos - posi_cos + self.m)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_data(best_acc, valid_acc_ls, dict_data, validIDlist, DFold_model, train_tfm, batch_size, device, epoch, K=5):\n",
    "    \n",
    "    valid_pair_dir = \"/home/wngys/lab/DeepFold/pair/pair_bool_90/\"\n",
    "    DFold_model.eval()\n",
    "    acc_num = 0\n",
    "    for id in validIDlist:\n",
    "        feature1 = get_feature(dict_data, id, train_tfm)\n",
    "        feature1 = feature1.to(device)\n",
    "\n",
    "        id_list = get_id_list(valid_pair_dir + id +\".txt\")\n",
    "        train_ds = Train_set(dict_data, id_list, train_tfm)\n",
    "        train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "        topKList = []\n",
    "        for feature2, label in train_dl:\n",
    "            with torch.no_grad():\n",
    "                fingerpvec1 = DFold_model(feature1)\n",
    "            feature2 = feature2.to(device)\n",
    "            label = label.to(device)\n",
    "            fingerBatch = fingerpvec1\n",
    "            for b_i in range(feature2.shape[0] - 1):\n",
    "                fingerBatch = torch.cat((fingerBatch,fingerpvec1), dim = 0)\n",
    "            with torch.no_grad():\n",
    "                fingerpvec2 = DFold_model(feature2)\n",
    "\n",
    "            cos_smi_batch = F.cosine_similarity(fingerBatch, fingerpvec2, dim=-1)\n",
    "            for cos_smi_idx in range(cos_smi_batch.shape[0]):\n",
    "                if(len(topKList) < K):\n",
    "                    topKList.append((cos_smi_batch[cos_smi_idx], label[cos_smi_idx]))\n",
    "                else:\n",
    "                    min_value = min(topKList)\n",
    "                    if (cos_smi_batch[cos_smi_idx], label[cos_smi_idx]) > min_value:\n",
    "                        min_idx = topKList.index(min_value)\n",
    "                        topKList[min_idx] = (cos_smi_batch[cos_smi_idx], label[cos_smi_idx])\n",
    "        acc_flag = False\n",
    "        for _, label in topKList:\n",
    "            if label == 1:\n",
    "                acc_flag = True\n",
    "        if acc_flag:\n",
    "            acc_num += 1\n",
    "\n",
    "    acc = acc_num / len(validIDlist)\n",
    "    valid_acc_ls.append(acc)\n",
    "    print(f\"Epoch: {epoch} | acc_num: {acc_num} | total_num: {len(validIDlist)} | acc: {acc_num / len(validIDlist):.4f}\")\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(DFold_model.state_dict(), \"/home/wngys/lab/DeepFold/model/best_model.pt\")\n",
    "        print(f\"saving best model with acc: {best_acc:.4f}\")    \n",
    "    DFold_model.train()\n",
    "    return best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DFold_model = DeepFold(in_channel = 3)\n",
    "DFold_model.to(device)\n",
    "\n",
    "train_tfm = build_transform(in_channel = 3)\n",
    "optimizer = torch.optim.SGD(DFold_model.parameters(), lr = 1e-4)\n",
    "lossF = Max_margin_loss(K = 10, m = 0.1)\n",
    "\n",
    "total_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "train_loss_ls = []\n",
    "valid_acc_ls = []\n",
    "\n",
    "pair_dir = \"/home/wngys/lab/DeepFold/pair/train_pair_bool_90/\"  \n",
    "valid_pair_dir = \"/home/wngys/lab/DeepFold/pair/pair_bool_90/\"\n",
    "\n",
    "trainIDlist = np.load(\"/home/wngys/lab/DeepFold/pair/train.npy\", allow_pickle=True)\n",
    "random.shuffle(trainIDlist)\n",
    "validIDlist = np.load(\"/home/wngys/lab/DeepFold/pair/valid.npy\", allow_pickle=True)\n",
    "random.shuffle(validIDlist)\n",
    "validIDlist = validIDlist[:100]\n",
    "\n",
    "dict_data = np.load(\"/home/wngys/lab/DeepFold/distance_matrix_r/matrix_data.npy\", allow_pickle=True).tolist()\n",
    "resume_dir = None\n",
    "# resume_dir = \"/home/wngys/lab/DeepFold/model/model_5.pt\"\n",
    "if resume_dir is not None:\n",
    "    chkp = torch.load(\"/home/wngys/lab/DeepFold/model/model_5.pt\")\n",
    "    st_epoch = chkp[\"epoch\"]\n",
    "    best_acc = chkp[\"best_acc\"]\n",
    "    train_loss_ls.extend(chkp[\"train_loss_ls\"])\n",
    "    valid_acc_ls.extend(chkp[\"valid_acc_ls\"])\n",
    "    DFold_model.load_state_dict(chkp[\"model_param\"])\n",
    "    optimizer.load_state_dict(chkp[\"optim_param\"])\n",
    "    validIDlist = chkp[\"valid_id_list\"]\n",
    "else:\n",
    "    st_epoch = 0\n",
    "    best_acc = 0\n",
    "\n",
    "\n",
    "for epoch in range(st_epoch, total_epochs):\n",
    "    # 遍历左侧一列集合每一个Protein ID\n",
    "    DFold_model.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # random.shuffle(trainIDlist)\n",
    "    for id_idx, id in enumerate(trainIDlist[:400]):\n",
    "        feature1 = get_feature(dict_data, id, train_tfm)\n",
    "        feature1 = feature1.to(device)\n",
    "\n",
    "        id_list = get_id_list(pair_dir + id +\".txt\")\n",
    "        train_ds = Train_set(dict_data, id_list, train_tfm)\n",
    "        train_dl = DataLoader(train_ds, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        IDtotalLoss = 0\n",
    "        for feature2, label in train_dl:\n",
    "            fingerpvec1 = DFold_model(feature1)\n",
    "            fingerpvec1 = fingerpvec1.squeeze(0)\n",
    "            feature2 = feature2.to(device)\n",
    "            label = label.to(device)\n",
    "            fingerpvec2 = DFold_model(feature2)\n",
    "            \n",
    "            loss = lossF(fingerpvec1, fingerpvec2, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            IDtotalLoss += loss.item()\n",
    "        \n",
    "        total_train_loss += IDtotalLoss\n",
    "\n",
    "        print(f\"Epoch: {epoch} | IDidx: {id_idx} | queryID: {id} | avg_loss: {IDtotalLoss / len(train_ds):.4f} | pair_num: {len(train_ds)}\")\n",
    "\n",
    "        if (id_idx + 1) % 200 == 0:\n",
    "            best_acc = valid_data(best_acc, valid_acc_ls, dict_data, validIDlist, DFold_model, train_tfm, batch_size, device, epoch, K=5)\n",
    "\n",
    "    train_loss_ls.append(total_train_loss)\n",
    "    print(f\"Epoch: {epoch} | total_loss: {total_train_loss:.4f}\")\n",
    "\n",
    "    chkp = {\n",
    "        \"epoch\": epoch+1,\n",
    "        \"best_acc\": best_acc,\n",
    "        \"model_param\": DFold_model.state_dict(),\n",
    "        \"optim_param\": optimizer.state_dict(),\n",
    "        \"train_loss_ls\": train_loss_ls,\n",
    "        \"valid_acc_ls\": valid_acc_ls,\n",
    "        \"valid_id_list\": validIDlist\n",
    "    }\n",
    "    torch.save(chkp, \"/home/wngys/lab/DeepFold/model/\" + f\"model_{epoch}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "acc_num: 242 | total: 500 | acc: 0.4840\n"
     ]
    }
   ],
   "source": [
    "# 测试过程\n",
    "# 测试集前五百ID 157/500 训练后的模型 acc 31.4% | 随机模型 226/500 acc 45%\n",
    "# 训练集前五百ID 160/500 acc 32%  | 随机模型 242/500 acc 48%\n",
    "# 完全没有学到东西\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4, 1, 2, 3\"\n",
    "device_ids = [0, 1, 2, 3]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DFold_model = DeepFold(in_channel = 3)\n",
    "# DFold_model.to(device)\n",
    "DFold_model = nn.DataParallel(DFold_model, device_ids).to(device)\n",
    "# chkp = torch.load(\"/home/wngys/lab/DeepFold/model/model_10_400_100/model_9.pt\")\n",
    "# DFold_model.load_state_dict(chkp[\"model_param\"])\n",
    "\n",
    "train_tfm = build_transform(in_channel = 3)\n",
    "\n",
    "test_pair_dir = \"/home/wngys/lab/DeepFold/pair/pair_bool_90/\"\n",
    "testIDlist = np.load(\"/home/wngys/lab/DeepFold/pair/train.npy\", allow_pickle=True)[:500]\n",
    "dict_data = np.load(\"/home/wngys/lab/DeepFold/distance_matrix_r/matrix_data.npy\", allow_pickle=True).tolist()\n",
    "\n",
    "K = 5\n",
    "batch_size = 64\n",
    "DFold_model.eval()\n",
    "\n",
    "acc_num = 0\n",
    "acc_ID = []\n",
    "for i, id in enumerate(testIDlist):\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(i+1)\n",
    "    feature1 = get_feature(dict_data, id, train_tfm)\n",
    "    feature1 = feature1.to(device)\n",
    "\n",
    "    id_list = get_id_list(test_pair_dir + id +\".txt\")\n",
    "    train_ds = Train_set(dict_data, id_list, train_tfm)\n",
    "    train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "    topKList = []\n",
    "    for feature2, label in train_dl:\n",
    "        with torch.no_grad():\n",
    "            fingerpvec1 = DFold_model(feature1)\n",
    "        feature2 = feature2.to(device)\n",
    "        label = label.to(device)\n",
    "        fingerBatch = fingerpvec1\n",
    "        for b_i in range(feature2.shape[0] - 1):\n",
    "            fingerBatch = torch.cat((fingerBatch,fingerpvec1), dim = 0)\n",
    "        with torch.no_grad():\n",
    "            fingerpvec2 = DFold_model(feature2)\n",
    "\n",
    "        cos_smi_batch = F.cosine_similarity(fingerBatch, fingerpvec2, dim=-1)\n",
    "        for cos_smi_idx in range(cos_smi_batch.shape[0]):\n",
    "            if(len(topKList) < K):\n",
    "                topKList.append((cos_smi_batch[cos_smi_idx], label[cos_smi_idx]))\n",
    "            else:\n",
    "                min_value = min(topKList)\n",
    "                if (cos_smi_batch[cos_smi_idx], label[cos_smi_idx]) > min_value:\n",
    "                    min_idx = topKList.index(min_value)\n",
    "                    topKList[min_idx] = (cos_smi_batch[cos_smi_idx], label[cos_smi_idx])\n",
    "    acc_flag = False\n",
    "    for _, label in topKList:\n",
    "        if label == 1:\n",
    "            acc_flag = True\n",
    "    if acc_flag:\n",
    "        acc_num += 1\n",
    "        acc_ID.append(id)\n",
    "\n",
    "acc = acc_num / len(testIDlist)\n",
    "\n",
    "print(f\"acc_num: {acc_num} | total: {len(testIDlist)} | acc: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n"
     ]
    }
   ],
   "source": [
    "Dir = \"/home/wngys/lab/DeepFold/pair/pair_bool_90/\"\n",
    "\n",
    "print(len(acc_ID))   \n",
    "# for id in acc_ID:\n",
    "#     posi_data = []\n",
    "#     nega_data = []\n",
    "\n",
    "#     with open(Dir+id+\".txt\", \"r\") as f_r:\n",
    "#         while True:\n",
    "#             lines = f_r.readline()\n",
    "#             if not lines:\n",
    "#                 break\n",
    "#             # id2 = lines.split('\\t')[0]\n",
    "#             flag = lines.split('\\t')[1].split(\"\\n\")[0]\n",
    "#             # print(type(flag))\n",
    "#             if flag == \"1\":\n",
    "#                 posi_data.append(lines)\n",
    "#             else:\n",
    "#                 nega_data.append(lines)\n",
    "    \n",
    "#     posi_num = len(posi_data)\n",
    "#     nega_num = len(nega_data)\n",
    "#     total_num = posi_num + nega_num\n",
    "\n",
    "#     print(f\"{id} | {posi_num} | {nega_num} | {total_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1zcha1 | 9 | 887 | 896\n",
      "d1mj4a_ | 3 | 614 | 617\n",
      "d1u9da1 | 11 | 1794 | 1805\n",
      "d1rtqa_ | 12 | 2461 | 2473\n",
      "d1pp9a1 | 4 | 1584 | 1588\n",
      "d1su3a2 | 1 | 275 | 276\n",
      "d1uewa1 | 17 | 253 | 270\n",
      "d2cura2 | 8 | 34 | 42\n",
      "d2gqfa2 | 1 | 269 | 270\n",
      "d1oz2a1 | 5 | 279 | 284\n",
      "d4a63b2 | 5 | 131 | 136\n",
      "d2glia1 | 3 | 4412 | 4415\n",
      "d1y9ka1 | 31 | 1670 | 1701\n",
      "d3zhga_ | 2 | 84 | 86\n",
      "d2zxea4 | 2 | 660 | 662\n",
      "d1wspa1 | 2 | 825 | 827\n",
      "d2r55a1 | 1 | 658 | 659\n",
      "d2uyya1 | 4 | 1582 | 1586\n",
      "d4agka_ | 9 | 25 | 34\n",
      "d1aqza_ | 2 | 225 | 227\n",
      "d2x1la2 | 2 | 505 | 507\n",
      "d4yw2a1 | 6 | 63 | 69\n",
      "d2cpwa1 | 10 | 1032 | 1042\n",
      "d1v5ua1 | 17 | 503 | 520\n",
      "d3o63a1 | 41 | 456 | 497\n",
      "d2rg7a_ | 4 | 601 | 605\n",
      "d1ow0a1 | 10 | 1664 | 1674\n",
      "d2nwua1 | 2 | 258 | 260\n",
      "d4mypa_ | 1 | 159 | 160\n",
      "d1gu7a2 | 13 | 2928 | 2941\n",
      "d2jexa_ | 1 | 291 | 292\n",
      "d3v8da1 | 15 | 258 | 273\n",
      "d5gzca_ | 1 | 45 | 46\n",
      "d5oolw_ | 11 | 33 | 44\n",
      "d3pl2a_ | 7 | 687 | 694\n",
      "d1i7ka_ | 10 | 859 | 869\n",
      "d4z4ja1 | 2 | 64 | 66\n",
      "d2y9ma1 | 2 | 270 | 272\n",
      "d1gkpa1 | 4 | 89 | 93\n",
      "d4po5a1 | 2 | 92 | 94\n",
      "d3oida1 | 64 | 858 | 922\n",
      "d2bzla_ | 10 | 1248 | 1258\n",
      "d5m0na1 | 3 | 26 | 29\n",
      "d2fm8a1 | 7 | 1238 | 1245\n",
      "d1x9ma2 | 3 | 1510 | 1513\n",
      "d1xeoa_ | 2 | 350 | 352\n",
      "d2he7a1 | 10 | 832 | 842\n",
      "d3toya1 | 7 | 213 | 220\n",
      "d2q7fa1 | 9 | 2107 | 2116\n",
      "d3t6ka_ | 13 | 962 | 975\n",
      "d1et0a_ | 7 | 791 | 798\n",
      "d3grca_ | 24 | 1362 | 1386\n",
      "d2l2fa_ | 1 | 173 | 174\n",
      "d1gh2a_ | 19 | 2415 | 2434\n",
      "d3kpba_ | 4 | 115 | 119\n",
      "d1vi6a_ | 28 | 1696 | 1724\n",
      "d4r2va1 | 3 | 160 | 163\n",
      "d3l8aa_ | 2 | 769 | 771\n",
      "d3p0ta1 | 3 | 332 | 335\n",
      "d2r85a2 | 8 | 163 | 171\n",
      "d1vlja1 | 3 | 2105 | 2108\n",
      "d4aoha_ | 1 | 45 | 46\n",
      "d2id3a1 | 8 | 2122 | 2130\n",
      "d3qk9a_ | 6 | 111 | 117\n",
      "d2aeea1 | 2 | 1469 | 1471\n",
      "d2g3wa1 | 2 | 1293 | 1295\n",
      "d3c3ka1 | 45 | 1034 | 1079\n",
      "d1pgua2 | 24 | 406 | 430\n",
      "d2lj0a_ | 10 | 398 | 408\n",
      "d5gota1 | 37 | 86 | 123\n",
      "d4iwna_ | 6 | 348 | 354\n",
      "d2a7ka1 | 14 | 1002 | 1016\n",
      "d2guda1 | 2 | 58 | 60\n",
      "d3na8a_ | 6 | 555 | 561\n",
      "d2cz9a2 | 2 | 847 | 849\n",
      "d3ceia2 | 2 | 215 | 217\n",
      "d1h1oa1 | 4 | 1184 | 1188\n",
      "d2leba_ | 19 | 652 | 671\n",
      "d2f71a_ | 7 | 1103 | 1110\n",
      "d2z8la1 | 2 | 149 | 151\n",
      "d3b95a1 | 10 | 133 | 143\n",
      "d3ftba_ | 17 | 717 | 734\n",
      "d4og1a_ | 5 | 85 | 90\n",
      "d2irfg_ | 2 | 392 | 394\n",
      "d3jxga_ | 7 | 92 | 99\n",
      "d1vjxa1 | 20 | 1027 | 1047\n",
      "d2bkma_ | 2 | 1201 | 1203\n",
      "d1cvja1 | 26 | 3041 | 3067\n",
      "d2f5ya1 | 23 | 394 | 417\n",
      "d4cj9a_ | 9 | 16 | 25\n",
      "d1wgpa1 | 6 | 219 | 225\n",
      "d1tiqa1 | 13 | 1105 | 1118\n",
      "d2r2za1 | 1 | 525 | 526\n",
      "d1o59a2 | 6 | 1318 | 1324\n",
      "d3dgba2 | 21 | 588 | 609\n",
      "d1cnna_ | 4 | 13 | 17\n",
      "d2fxua1 | 3 | 1440 | 1443\n",
      "d2cofa1 | 9 | 397 | 406\n",
      "d2c3na1 | 9 | 2063 | 2072\n",
      "d5m0yb2 | 17 | 12 | 29\n",
      "d2q13a2 | 7 | 411 | 418\n",
      "d3en8a1 | 13 | 197 | 210\n",
      "d3eh8a1 | 1 | 218 | 219\n",
      "d2o7ta1 | 3 | 1558 | 1561\n",
      "d4ztka_ | 2 | 87 | 89\n",
      "d4b8ja_ | 3 | 127 | 130\n",
      "d1nyca1 | 2 | 1082 | 1084\n",
      "d4kjfa_ | 2 | 255 | 257\n",
      "d3wara_ | 19 | 213 | 232\n",
      "d2crza1 | 18 | 865 | 883\n",
      "d1re5a_ | 5 | 1565 | 1570\n",
      "d2dena_ | 5 | 2329 | 2334\n",
      "d1nppa2 | 11 | 1744 | 1755\n",
      "d1e5da2 | 3 | 1682 | 1685\n",
      "d1odka_ | 5 | 1877 | 1882\n",
      "d1nyra2 | 1 | 1102 | 1103\n",
      "d2e5fa_ | 5 | 1264 | 1269\n",
      "d1z0na1 | 1 | 932 | 933\n",
      "d1th7a1 | 13 | 673 | 686\n",
      "d4prsa_ | 4 | 96 | 100\n",
      "d4r8xa1 | 2 | 37 | 39\n",
      "d3rota_ | 14 | 640 | 654\n",
      "d4d9ba_ | 5 | 372 | 377\n",
      "d1k66a1 | 9 | 3526 | 3535\n",
      "d4h2wc1 | 3 | 322 | 325\n",
      "d3b8la1 | 11 | 170 | 181\n",
      "d2zdha2 | 4 | 256 | 260\n",
      "d2qalp1 | 1 | 534 | 535\n",
      "d4q4w3_ | 1 | 101 | 102\n",
      "d1wwma1 | 10 | 723 | 733\n",
      "d1x54a2 | 3 | 1144 | 1147\n",
      "d2xz4a_ | 1 | 412 | 413\n",
      "d2ws93_ | 3 | 446 | 449\n",
      "d2b5la1 | 15 | 357 | 372\n",
      "d5hdwa1 | 4 | 38 | 42\n",
      "d4p47a1 | 8 | 190 | 198\n",
      "d3saoa1 | 5 | 306 | 311\n",
      "d2h7za1 | 4 | 90 | 94\n",
      "d4nvra_ | 12 | 204 | 216\n",
      "d1hxma1 | 58 | 1646 | 1704\n",
      "d4inwa_ | 2 | 121 | 123\n",
      "d1k9oi_ | 16 | 1433 | 1449\n",
      "d1e8ca1 | 2 | 2814 | 2816\n",
      "d1feca3 | 2 | 481 | 483\n",
      "d1w66a1 | 6 | 1452 | 1458\n",
      "d2dj7a1 | 3 | 170 | 173\n",
      "d4e9sa2 | 3 | 132 | 135\n",
      "d2x7ba_ | 8 | 610 | 618\n",
      "d2owna2 | 19 | 262 | 281\n",
      "d1lj8a3 | 1 | 1343 | 1344\n",
      "d5aooc_ | 3 | 65 | 68\n",
      "d1c0pa1 | 9 | 1684 | 1693\n",
      "d3bb9a1 | 12 | 169 | 181\n",
      "d1lr7a1 | 1 | 304 | 305\n",
      "d5elna1 | 1 | 15 | 16\n",
      "d2gvka1 | 3 | 639 | 642\n",
      "d2vk9a1 | 1 | 747 | 748\n",
      "d4oana1 | 8 | 161 | 169\n",
      "d2jj7a2 | 13 | 845 | 858\n",
      "d1foea1 | 6 | 1218 | 1224\n",
      "d1psua_ | 9 | 931 | 940\n",
      "d2ra2a1 | 1 | 589 | 590\n",
      "d3nema2 | 2 | 288 | 290\n",
      "d1v7ra_ | 2 | 1254 | 1256\n",
      "d2q9oa3 | 1 | 449 | 450\n",
      "d2vc4a_ | 7 | 290 | 297\n",
      "d3i7ta_ | 4 | 936 | 940\n",
      "d1qnxa1 | 2 | 548 | 550\n",
      "d3moya_ | 23 | 243 | 266\n",
      "d4ba8a1 | 7 | 299 | 306\n",
      "d1dw0a_ | 1 | 555 | 556\n",
      "d1y5ia2 | 4 | 2370 | 2374\n",
      "d1pz7a_ | 7 | 706 | 713\n",
      "d2v9tb_ | 2 | 135 | 137\n",
      "d1w2za1 | 3 | 993 | 996\n",
      "d3mezb_ | 1 | 71 | 72\n",
      "d3un1a1 | 52 | 667 | 719\n",
      "d2or7a1 | 2 | 640 | 642\n",
      "d3b0fa_ | 3 | 2347 | 2350\n",
      "d4k6oa_ | 2 | 209 | 211\n",
      "d2b06a1 | 8 | 205 | 213\n",
      "d3m62b_ | 7 | 544 | 551\n",
      "d1cuna2 | 6 | 2538 | 2544\n",
      "d4pbha1 | 4 | 140 | 144\n",
      "d5ng0a_ | 2 | 11 | 13\n",
      "d2ayta1 | 4 | 1605 | 1609\n",
      "d3eura1 | 1 | 655 | 656\n",
      "d2c1ia1 | 5 | 1237 | 1242\n",
      "d2beca_ | 2 | 274 | 276\n",
      "d2cs0a1 | 14 | 955 | 969\n",
      "d2oo2a1 | 3 | 2079 | 2082\n",
      "d1ecfa2 | 2 | 1790 | 1792\n",
      "d1w27a_ | 3 | 1851 | 1854\n",
      "d2kcra_ | 3 | 88 | 91\n",
      "d5o95a2 | 17 | 36 | 53\n",
      "d2edva1 | 22 | 350 | 372\n",
      "d2cdqa1 | 2 | 1548 | 1550\n",
      "d1ehxa_ | 98 | 1234 | 1332\n",
      "d1yj5a2 | 7 | 1861 | 1868\n",
      "d2q4wa2 | 9 | 374 | 383\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Dir = \"/home/wngys/lab/DeepFold/pair/pair_bool_90/\"\n",
    "\n",
    "    \n",
    "for id in acc_ID:\n",
    "    posi_data = []\n",
    "    nega_data = []\n",
    "\n",
    "    with open(Dir+id+\".txt\", \"r\") as f_r:\n",
    "        while True:\n",
    "            lines = f_r.readline()\n",
    "            if not lines:\n",
    "                break\n",
    "            # id2 = lines.split('\\t')[0]\n",
    "            flag = lines.split('\\t')[1].split(\"\\n\")[0]\n",
    "            # print(type(flag))\n",
    "            if flag == \"1\":\n",
    "                posi_data.append(lines)\n",
    "            else:\n",
    "                nega_data.append(lines)\n",
    "    \n",
    "    posi_num = len(posi_data)\n",
    "    nega_num = len(nega_data)\n",
    "    total_num = posi_num + nega_num\n",
    "\n",
    "    print(f\"{id} | {posi_num} | {nega_num} | {total_num}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'best_acc', 'model_param', 'optim_param', 'train_loss_ls', 'valid_acc_ls', 'train_acc_ls', 'valid_id_list'])\n",
      "0.38\n",
      "[0.34, 0.34, 0.38, 0.33, 0.36, 0.33, 0.37, 0.32, 0.36, 0.29]\n",
      "[0.3, 0.2925, 0.2825, 0.3, 0.2625, 0.2975, 0.2475, 0.2875, 0.26, 0.2775]\n",
      "[9121.327145934105, 8711.894099235535, 8653.299540162086, 8631.024267792702, 8603.476480603218]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "chkp = torch.load(\"/home/wngys/lab/DeepFold/model/model_10_400_100/model_4.pt\")\n",
    "print(chkp.keys())\n",
    "\n",
    "losses = chkp[\"train_loss_ls\"]\n",
    "acces = chkp[\"valid_acc_ls\"]\n",
    "train_accs = chkp[\"train_acc_ls\"]\n",
    "b_acc = chkp[\"best_acc\"]\n",
    "\n",
    "print(b_acc)\n",
    "print(acces)\n",
    "print(train_accs)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理部分\n",
    "# 由于每个IDleft都要读取 对应的所有IDright对应的npy文件 大量IO速度过慢\n",
    "# 将所有ID对应的数据文件保存在一个字典，一开始就加载进内存\n",
    "def read_data():\n",
    "    data_dir = \"/home/wngys/lab/DeepFold/distance_matrix_r/distance_matrix_mine_r_3\"\n",
    "    data_dict = {}\n",
    "    for file_name in os.listdir(data_dir):\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        data = np.load(file_path, allow_pickle=True)\n",
    "        ID = file_name.split('.')[0]\n",
    "        data_dict[ID] = data\n",
    "\n",
    "    fileName = \"/home/wngys/lab/DeepFold/distance_matrix_r/matrix_data.npy\"\n",
    "    \n",
    "    np.save(fileName, data_dict)\n",
    "\n",
    "read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0014, dtype=torch.float64)\n",
      "tensor(-0.0248, dtype=torch.float64)\n",
      "tensor(-1.9978e-06, dtype=torch.float64)\n",
      "***********************************\n",
      "tensor(0.9975, dtype=torch.float64)\n",
      "tensor(0.9763, dtype=torch.float64)\n",
      "tensor(0.9537, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理部分 计算整体三个channel mean 和 std 用于normalize\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "Dir_path = \"/home/wngys/lab/DeepFold/distance_matrix_r/distance_matrix_mine_r_3\"\n",
    "total_mean0 = 0\n",
    "total_mean1 = 0\n",
    "total_mean2 = 0\n",
    "total_std0 = 0\n",
    "total_std1 = 0\n",
    "total_std2 = 0\n",
    "tfm = build_transform(in_channel=3)\n",
    "num = len(os.listdir(Dir_path))\n",
    "for file_name in os.listdir(Dir_path):\n",
    "    file_path = os.path.join(Dir_path, file_name)\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    data = torch.from_numpy(data)\n",
    "    data = tfm(data)\n",
    "    total_mean0 += data[0].mean()\n",
    "    total_mean1 += data[1].mean()\n",
    "    total_mean2 += data[2].mean()\n",
    "    total_std0 += data[0].std()\n",
    "    total_std1 += data[1].std()\n",
    "    total_std2 += data[2].std()\n",
    "avg_mean0 = total_mean0/num\n",
    "avg_mean1 = total_mean1/num\n",
    "avg_mean2 = total_mean2/num\n",
    "\n",
    "avg_std0 = total_std0/num\n",
    "avg_std1 = total_std1/num\n",
    "avg_std2 = total_std2/num\n",
    "\n",
    "print(avg_mean0)\n",
    "print(avg_mean1)\n",
    "print(avg_mean2)\n",
    "print(\"***********************************\")\n",
    "print(avg_std0)\n",
    "print(avg_std1)\n",
    "print(avg_std2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num:14274\n",
      "tensor(0.0068, dtype=torch.float64)\n",
      "tensor(0.0003, dtype=torch.float64)\n",
      "tensor(2.3069e-05, dtype=torch.float64)\n",
      "***********************************\n",
      "tensor(0.0140, dtype=torch.float64)\n",
      "tensor(0.0015, dtype=torch.float64)\n",
      "tensor(0.0002, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/home/wngys/lab/DeepFold/distance_matrix_r/matrix_data.npy\"\n",
    "\n",
    "dict_data = np.load(file_path, allow_pickle=True).tolist()\n",
    "print(f\"num:{len(dict_data.keys())}\")\n",
    "num = len(dict_data.keys())\n",
    "\n",
    "total_mean0 = 0\n",
    "total_mean1 = 0\n",
    "total_mean2 = 0\n",
    "total_std0 = 0\n",
    "total_std1 = 0\n",
    "total_std2 = 0\n",
    "\n",
    "for Id in dict_data.keys():\n",
    "    data = dict_data[Id]\n",
    "    data = torch.from_numpy(data)\n",
    "    total_mean0 += data[0].mean()\n",
    "    total_mean1 += data[1].mean()\n",
    "    total_mean2 += data[2].mean()\n",
    "    total_std0 += data[0].std()\n",
    "    total_std1 += data[1].std()\n",
    "    total_std2 += data[2].std()\n",
    "\n",
    "avg_mean0 = total_mean0/num\n",
    "avg_mean1 = total_mean1/num\n",
    "avg_mean2 = total_mean2/num\n",
    "\n",
    "avg_std0 = total_std0/num\n",
    "avg_std1 = total_std1/num\n",
    "avg_std2 = total_std2/num\n",
    "\n",
    "print(avg_mean0)\n",
    "print(avg_mean1)\n",
    "print(avg_mean2)\n",
    "print(\"***********************************\")\n",
    "print(avg_std0)\n",
    "print(avg_std1)\n",
    "print(avg_std2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/wngys/lab/DeepFold/Code/Net.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617461534349227d/home/wngys/lab/DeepFold/Code/Net.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m lossF \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(l1_c[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m l1_c[\u001b[39m1\u001b[39m], torch\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m,dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617461534349227d/home/wngys/lab/DeepFold/Code/Net.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m lossF \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617461534349227d/home/wngys/lab/DeepFold/Code/Net.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m lossF\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617461534349227d/home/wngys/lab/DeepFold/Code/Net.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(t\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[0;32m/opt/miniconda38/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/miniconda38/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# lossFt = nn.MSELoss()\n",
    "\n",
    "import torch\n",
    "t = torch.tensor([[1,1], [2,4], [1, 2], [2, 2]], dtype = torch.float, requires_grad=True)\n",
    "\n",
    "# lossFt = t[0][0] + 2* t[0][1]\n",
    "\n",
    "# lossFt.backward()\n",
    "# print(t.grad)\n",
    "\n",
    "l1 = []\n",
    "l2 = []\n",
    "l1.append(t[1])\n",
    "l1.append(t[3])\n",
    "l2.append(t[0])\n",
    "l2.append(t[2])\n",
    "\n",
    "l1_c = []\n",
    "l2_c = []\n",
    "\n",
    "v = torch.tensor([1, 2], dtype=torch.float)\n",
    "\n",
    "l1_c.append(F.cosine_similarity(v, l1[0], dim = 0))\n",
    "l1_c.append(F.cosine_similarity(v, l1[1], dim = 0))\n",
    "# print(l1_c)\n",
    "l1_c.sort()\n",
    "\n",
    "lossF = max(l1_c[0] - l1_c[1], torch.tensor(0,dtype=torch.float, requires_grad=True).detach())\n",
    "lossF += 1\n",
    "lossF.backward()\n",
    "print(t.grad)\n",
    "# l2_c.append(F.cosine_similarity)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('miniconda38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "474a727007314fe946f45bbc9b266c6bb0e4e19682495744a6293558354c7c6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
