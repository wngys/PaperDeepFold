{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "kernel_List = [12, 4, 4, 4, 4, 4]\n",
    "channel_List = [128, 256, 512, 512, 512, 400]\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_sz, padding, stride = 2) -> None:\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channel, out_channel, kernel_sz, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "def get_convBlocks(in_channel):\n",
    "    layerNum = len(kernel_List)\n",
    "    blocks = []\n",
    "    blocks.append(ConvBlock(in_channel, channel_List[0], kernel_List[0], int(kernel_List[0] / 2 - 1)))\n",
    "    for i in range(1, layerNum):\n",
    "        blocks.append(ConvBlock(channel_List[i-1], channel_List[i], kernel_List[i], int(kernel_List[i] / 2 - 1)))\n",
    "    return blocks\n",
    "\n",
    "class DeepFold(nn.Module):\n",
    "    def __init__(self, in_channel) -> None:\n",
    "        super().__init__()\n",
    "        self.convLayer = nn.Sequential(*get_convBlocks(in_channel))\n",
    "    \n",
    "    # [batch_size, 3, 256, 256]\n",
    "    def forward(self, x):\n",
    "        # [batch_size, 400, 4, 4]\n",
    "        x = self.convLayer(x)\n",
    "        # [batch_size, 400, 4]\n",
    "        x = torch.diagonal(x, dim1=2, dim2=3)\n",
    "        # [batch_size, 400]\n",
    "        x = torch.mean(x, dim= 2)\n",
    "\n",
    "        normValue = torch.norm(x, dim = 1) # norm_value [batch_size]\n",
    "        # print(normValue.shape)\n",
    "        # [400, batch_size]  除法要求最后一维要和norm_value维度匹配\n",
    "        x = x.permute(1, 0)\n",
    "        # [400, batch_size] 已经正则化\n",
    "        x = torch.div(x, normValue)\n",
    "\n",
    "        # [batch_size, 400]\n",
    "        x = x.permute(1, 0)\n",
    "        return x\n",
    "\n",
    "    # def hook(self, layer: nn.Module, input: torch.tensor, output)\n",
    "\n",
    "# outputList = []\n",
    "# def hook(self, layer: nn.Module,  output: torch.tensor):\n",
    "#     outputList.append(output)\n",
    "\n",
    "x = torch.rand(2, 3, 256, 256)\n",
    "\n",
    "model = DeepFold(3)\n",
    "\n",
    "# for layer in model.convLayer:\n",
    "#     layer.register_forward_hook(hook)\n",
    "\n",
    "y = model(x)\n",
    "\n",
    "print(torch.norm(y, dim = 1).shape)\n",
    "\n",
    "# for ele in outputList:\n",
    "#     print(ele.shape)\n",
    "\n",
    "# print(model)\n",
    "# print(help(model))\n",
    "# print(len(list(model.named_modules())))\n",
    "# for name,_ in model.convLayer.named_modules():\n",
    "#     print(name)\n",
    "#     print('-'*60)\n",
    "\n",
    "# √\n",
    "# print(x.shape)\n",
    "# for layer in model.convLayer:\n",
    "#     x = layer(x)\n",
    "#     print(x.shape)\n",
    "#     print('-'*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6000, 0.8000],\n",
      "        [0.3846, 0.9231],\n",
      "        [0.6000, 0.8000]])\n",
      "tensor([ 5., 13.,  5.])\n",
      "tensor([[ 3.,  5.,  3.],\n",
      "        [ 4., 12.,  4.]])\n",
      "tensor([[0.6000, 0.3846, 0.6000],\n",
      "        [0.8000, 0.9231, 0.8000]])\n",
      "tensor([[0.6000, 0.8000],\n",
      "        [0.3846, 0.9231],\n",
      "        [0.6000, 0.8000]])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "x = torch.tensor(\n",
    "    [[3, 4],\n",
    "    [5, 12],\n",
    "    [3, 4]],dtype=torch.float32)\n",
    "# print(x.dtype)\n",
    "print(F.normalize(x))\n",
    "\n",
    "normV = torch.norm(x, dim = 1)\n",
    "print(normV)\n",
    "x = x.permute(1, 0)\n",
    "print(x)\n",
    "res = torch.div(x, normV)\n",
    "print(res)\n",
    "res = res.permute(1, 0)\n",
    "print(res)\n",
    "normV2 = torch.norm(res, dim = 1)\n",
    "print(normV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcy(x: torch.tensor, k:int):\n",
    "    return x**(-2*k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pretfm(torch.nn.Module):\n",
    "    def __init__(self, in_channel) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channel = in_channel\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "\n",
    "        y = torch.rand(self.in_channel, 256, 256)\n",
    "\n",
    "        for i in range(1, self.in_channel+1):\n",
    "            y[i-1] = funcy(x, i)\n",
    "        x = y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channel = 3\n",
    "import torchvision.transforms as T\n",
    "train_tfm = T.Compose(\n",
    "    [\n",
    "        T.Resize((256, 256)),\n",
    "        # 取逆矩阵 扩充channel\n",
    "        # Pretfm(in_channel),\n",
    "        # 是否需要数据增强 保留一个问号\n",
    "        # 层归一化\n",
    "        # nn.LayerNorm((in_channel, 256, 256))\n",
    "    ]\n",
    ")\n",
    "# t = nn.LayerNorm((in_channel, 256, 256))\n",
    "# # x = torch.rand(1, 188, 188)\n",
    "# x = torch.tensor([[[1,1],\n",
    "#                     [2,2]\n",
    "#                     ]\n",
    "#                     ], dtype=torch.float32)\n",
    "# output = train_tfm(x)\n",
    "# print(output)\n",
    "# # print(output.shape)\n",
    "# print(t(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Train_set(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, dir, id_list, tfm) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.tensor_list = []\n",
    "        for id, label in id_list:\n",
    "            # 在蛋白质数据库文件查找 id.npy\n",
    "            feature = torch.from_numpy(np.load(dir+id+\".npy\", allow_pickle=True))\n",
    "            feature = torch.unsqueeze(feature, 0)\n",
    "            self.tensor_list.append((feature,\n",
    "                                        label)\n",
    "                                        )\n",
    "        self.tfm = tfm\n",
    "\n",
    "    def __getitem__(self, idx :int):\n",
    "        y = self.tensor_list[idx][0]\n",
    "        y = self.tfm(y)\n",
    "        label = torch.float64(self.tensor_list[idx][1])\n",
    "        return y, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d1a0pa2', '0'), ('d2a0ua1', '0'), ('d3a04a_', '0'), ('d5a0ya1', '0'), ('d5a0ya2', '0'), ('d1a1ia1', '0'), ('d1a1va2', '0'), ('d2a14a1', '0'), ('d2a19a2', '0'), ('d2a1jb1', '0')]\n",
      "torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "dir = \"../distance_matrix/distance_matrix_inf/\"\n",
    "id_path = \"../pair/pair_bool/d1a0aa_.txt\"\n",
    "id_list = []\n",
    "\n",
    "with open(id_path, \"r\") as f_r:\n",
    "    while True:\n",
    "        lines = f_r.readline()\n",
    "        if not lines:\n",
    "            break\n",
    "        line = lines.split('\\n')[0].split('\\t')\n",
    "        id_list.append((line[0], line[1]))\n",
    "\n",
    "# print(len(id_list))\n",
    "\n",
    "dataset = Train_set(dir, id_list[:10], train_tfm)\n",
    "print(id_list[:10])\n",
    "print(dataset[0][0].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"../distance_matrix/distance_matrix_inf/d1a0pa2.npy\", allow_pickle=True)\n",
    "data = torch.from_numpy(data)\n",
    "print(data.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取左侧一列id 对应的右侧id_list\n",
    "def get_id_list(pair_path):\n",
    "    id_list = []\n",
    "    with open(pair_path, \"r\") as f_r:\n",
    "        while True:\n",
    "            lines = f_r.readline()\n",
    "            if not lines:\n",
    "                break\n",
    "            line = lines.split('\\n')[0].split('\\t')\n",
    "            id_list.append((line[0], line[1]))\n",
    "\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取id对应的distance_matrix\n",
    "def get_feature(data_path, tfm):\n",
    "    feature = torch.from_numpy(np.load(data_path, allow_pickle=True))\n",
    "    feature = torch.unsqueeze(feature, 0)\n",
    "    feature = feature.to(torch.float)\n",
    "    feature = tfm(feature)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63, 9, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "lis = [3, 2,63,9]\n",
    "lis.sort(reverse=True)\n",
    "print(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(posi_cosList, nega_cosList, K = 10, m = 0.1):\n",
    "    posi_cosList.sort() # 升序排序 选最小\n",
    "    nega_cosList.sort(reverse=True) # 降序排序 选最大\n",
    "    posi_cos = posi_cosList[0] # 只选取一个正例\n",
    "    loss = 0\n",
    "    for i in range(K):\n",
    "        nega_cos = nega_cosList[i]\n",
    "        loss += max(0, nega_cos - posi_cos + m)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=None)\n",
      ")\n",
      "tensor([[[ 0.0000,  0.0000,  0.4402,  ..., 60.1548, 60.5247, 60.5247],\n",
      "         [ 0.0000,  0.0000,  0.4402,  ..., 60.1548, 60.5247, 60.5247],\n",
      "         [ 0.4402,  0.4402,  0.7789,  ..., 59.8829, 60.2524, 60.2524],\n",
      "         ...,\n",
      "         [60.1548, 60.1548, 59.8829,  ...,  0.7703,  0.4353,  0.4353],\n",
      "         [60.5247, 60.5247, 60.2524,  ...,  0.4353,  0.0000,  0.0000],\n",
      "         [60.5247, 60.5247, 60.2524,  ...,  0.4353,  0.0000,  0.0000]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ 0.0000,  2.1070,  4.2962,  ..., 33.5065, 34.7412, 35.4779],\n",
      "         [ 2.1070,  1.8766,  2.4689,  ..., 34.8320, 36.0934, 36.8219],\n",
      "         [ 4.2962,  2.4689,  1.4872,  ..., 35.3302, 36.6074, 37.3034],\n",
      "         ...,\n",
      "         [33.5065, 34.8320, 35.3302,  ...,  1.4567,  2.4249,  4.2229],\n",
      "         [34.7412, 36.0934, 36.6074,  ...,  2.4249,  1.8970,  2.1299],\n",
      "         [35.4779, 36.8219, 37.3034,  ...,  4.2229,  2.1299,  0.0000]]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/wngys/lab/DeepFold/Code/Net.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617461534349227d/home/wngys/lab/DeepFold/Code/Net.ipynb#ch0000012vscode-remote?line=45'>46</a>\u001b[0m \u001b[39mprint\u001b[39m(train_ds[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617461534349227d/home/wngys/lab/DeepFold/Code/Net.ipynb#ch0000012vscode-remote?line=46'>47</a>\u001b[0m train_dl \u001b[39m=\u001b[39m DataLoader(train_ds, batch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617461534349227d/home/wngys/lab/DeepFold/Code/Net.ipynb#ch0000012vscode-remote?line=48'>49</a>\u001b[0m fingerpvec1 \u001b[39m=\u001b[39m DFold_model(feature1)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617461534349227d/home/wngys/lab/DeepFold/Code/Net.ipynb#ch0000012vscode-remote?line=51'>52</a>\u001b[0m IDtotalLoss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617461534349227d/home/wngys/lab/DeepFold/Code/Net.ipynb#ch0000012vscode-remote?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m feature2, label \u001b[39min\u001b[39;00m train_dl:\n",
      "File \u001b[0;32m/opt/miniconda38/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/lab/DeepFold/Code/model.py:36\u001b[0m, in \u001b[0;36mDeepFold.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     35\u001b[0m     \u001b[39m# [batch_size, 400, 4, 4]\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvLayer(x)\n\u001b[1;32m     37\u001b[0m     \u001b[39m# [batch_size, 400, 4]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdiagonal(x, dim1\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, dim2\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda38/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/miniconda38/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda38/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/lab/DeepFold/Code/model.py:16\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)))\n\u001b[1;32m     17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[1;32m     18\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/miniconda38/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/miniconda38/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:135\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    137\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda38/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:407\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    406\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mexpected 4D input (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n",
      "\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "# 训练过程\n",
    "#/home/wngys/lab/DeepFold/Code\n",
    "from model import *\n",
    "from data import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cosine_similarity\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DFold_model = DeepFold(in_channel = 1)\n",
    "DFold_model.to(device)\n",
    "\n",
    "train_tfm = build_transform(in_channel=1)\n",
    "\n",
    "print(train_tfm)\n",
    "optimizer = torch.optim.SGD(DFold_model.parameters(), lr = 1e-3)\n",
    "\n",
    "total_epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "resume_dir = None\n",
    "if resume_dir is not None:\n",
    "    pass\n",
    "else:\n",
    "    st_epoch = 0\n",
    "\n",
    "pair_dir = \"home/wngys/lab/DeepFold/pair/train_pair_bool_90/\"  \n",
    "data_dir = \"../distance_matrix/distance_matrix_mine/\" \n",
    "\n",
    "trainIDset = [\"d1a0aa_\"]\n",
    "\n",
    "leftTrain_ds = LeftTrainSet(data_dir, trainIDset, train_tfm)\n",
    "leftTrain_dl = DataLoader(leftTrain_ds, batch_size, shuffle = True)\n",
    "\n",
    "for epoch in range(st_epoch, total_epochs):\n",
    "    # 遍历左侧一列集合每一个Protein ID\n",
    "    DFold_model.train()\n",
    "\n",
    "    for IDBatch, feature1 in leftTrain_dl:\n",
    "        feature1 = feature1.to(device)\n",
    "        fingerpbatch = DFold_model(feature1)\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            id_list = get_id_list(pair_dir + IDBatch[batch_idx] +\".txt\")\n",
    "            # id_list = get_id_list(\"/home/wngys/lab/DeepFold/test_d1a0aa_Pair.txt\")\n",
    "            fingerpvec1 = feature1[batch_idx]\n",
    "            # feature1 = get_feature(data_dir + id + \".npy\", train_tfm)\n",
    "\n",
    "            \n",
    "            # print(feature1)\n",
    "\n",
    "            train_ds = Train_set(data_dir, id_list, train_tfm)\n",
    "            # print(train_ds[0][0])\n",
    "            train_dl = DataLoader(train_ds, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "\n",
    "            IDtotalLoss = 0\n",
    "            for feature2, label in train_dl:\n",
    "                feature2 = feature2.to(device)\n",
    "                label = label.to(device)\n",
    "                fingerpvec2 = DFold_model(feature2)\n",
    "                \n",
    "                posi_vec_list = []\n",
    "                nega_vec_list = []\n",
    "\n",
    "                for number_inbatch in range(fingerpvec2.shape[0]):\n",
    "                    if label[number_inbatch] == 0:\n",
    "                        nega_vec_list.append(fingerpvec2[number_inbatch])\n",
    "                    elif label[number_inbatch] == 1:\n",
    "                        posi_vec_list.append(fingerpvec2[number_inbatch])\n",
    "                    else:\n",
    "                        print(\"ERROR\")\n",
    "\n",
    "                posi_cos_smi_list = []\n",
    "                nega_cos_smi_list = []\n",
    "\n",
    "                for posi_vec in posi_vec_list:\n",
    "                    posi_cos_smi_list.append(F.cosine_similarity(fingerpvec1, posi_vec, dim = 0))\n",
    "\n",
    "                for nega_vec in nega_vec_list:\n",
    "                    nega_cos_smi_list.append(F.cosine_similarity(fingerpvec1, nega_vec, dim = 0))\n",
    "\n",
    "                # 计算batch Loss\n",
    "                loss = compute_loss(posi_cos_smi_list, nega_cos_smi_list)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                IDtotalLoss += loss\n",
    "\n",
    "            print(f\"Epoch: {epoch} | avg_loss: {IDtotalLoss / len(train_dl):.4f}\")\n",
    "\n",
    "        # DFold_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9487, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "test_t = torch.tensor([1, 1], dtype=torch.float64)\n",
    "test_t2 = torch.tensor([2, 4], dtype = torch.float64)\n",
    "\n",
    "print(F.cosine_similarity(test_t, test_t2,dim = -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('miniconda38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "474a727007314fe946f45bbc9b266c6bb0e4e19682495744a6293558354c7c6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
